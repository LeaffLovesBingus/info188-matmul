# info188-matmul

## Introducci贸n

###### Integrantes:
- Eduardo Montecinos
- Crist贸bal Silva
- Diego Soto
- Mat铆as Soto
- Mat铆as Toledo

La multiplicaci贸n de matrices es una operaci贸n fundamental en la computaci贸n, como se nos ha mencionado en clases, para la simulaci贸n f铆sica y el entrenamiento de redes neuronales. Al tratarse de una operaci贸n con complejidad secuencial de orden c煤bico, la manera en que se implementa puede tener un impacto significativo en el rendimiento y eficiencia de los sistemas, con el riesgo de que se dispare el runtime en funci贸n del tama帽o de las entradas. Por ello, resulta crucial explorar opciones de paralelizaci贸n para reducir el tiempo de c贸mputo excesivo.

En esta tarea se implement贸 la multiplicaci贸n de matrices con diferentes aplicaciones de procesamiento en paralelo:
1. Paralelismo en CPU 
2. Paralelismo en GPU b谩sico (con el approach tratado en clases) 
3. Paralelismo en GPU con **memoria compartida** 
4. Paralelismo en GPU con **tensor cores** 

El paralelismo en CPU fue programado con OpenMP (`#pragma omp parallel for`), mientras que las otras tres opciones se trabajaron con CUDA (NVIDIA). Los cuatro enfoques se encuentran en el mismo archivo [`main.cu`](./main.cu), que se compila con `make` y se ejecuta como:
```bash
./prog <n tama帽o matriz> <n煤mero de threads CPU> <opci贸n 1-4 de algoritmos>
```

Con el fin de evaluar el rendimiento de cada opci贸n, se realizaron pruebas utilizando diferentes tama帽os de entrada (matrices cuadradas `n*n`) y se midi贸 el tiempo de ejecuci贸n en cada caso. Con los tiempos medidos, tambi茅n se determin贸 el speedup/aceleraci贸n de las tres implementaciones en GPU con respecto al paralelismo en CPU.

---

## Resultados

A partir de los experimentos realizados, se observa una diferencia de rendimiento sustancial entre la ejecuci贸n paralelizada en CPU y la ejecuci贸n con hilos en GPU.

### Salto de tiempos de ejecuci贸n CPU vs GPU

![](./img/CPU.png)

Como se evidencia en la tabla de resultados y en los gr谩ficos, el tiempo de ejecuci贸n en la CPU crece dr谩sticamente a medida que aumenta el tama帽o de la matriz (cuadrada `n*n`). En el primer gr谩fico se distingue que, al llegar a `n = 3072`, el runtime alcanza las centenas de segundos, mientras que en las tres implementaciones paralelizadas en GPU las curvas se mantienen considerablemente cercanas al eje de las abscisas (en esta escala gr谩fica resultan indistinguibles).

En el caso `n = 4096`, la implementaci贸n en CPU tard贸 **291.12 segundos**, mientras que la implementaci贸n b谩sica/directa en GPU tard贸 **0.29 segundos**. Este caso representa un speedup de aproximadamente **989x**. La brecha en runtime se debe no solo a que la CPU est谩 limitada por un n煤mero mucho menor de n煤cleos f铆sicos (6 n煤cleos, 12 hilos en el Ryzen 5 3600 utilizado), sino tambi茅n por la latencia de acceso a la memoria, contrastada con la capacidad superior de la GPU de explotar el paralelismo en cargas masivas de trabajo aritm茅tico.

### Memoria global y memoria compartida

![](./img/GPU.png)

En el segundo gr谩fico se observa que la implementaci贸n b谩sica en GPU (enfoque _naive_, l铆nea roja) se degrada en rendimiento m谩s r谩pido que las otras dos versiones a medida que aumenta `n`. Esto es porque la opci贸n b谩sica en GPU calcula las componentes de la matriz con accesos repetidos a la **memoria global** para obtener los valores en `A` y `B`. Como no existe reutilizaci贸n de datos entre hilos de la GPU, estos tardan m谩s en acceder a los datos de entrada que en calcular las componentes de salida.

La curva amarilla, por otro lado, exhibe una mejora notable en redimiento. El uso de _tiling_ (con `TILE_SIZE 16`) permite que los hilos de un bloque carguen bloques de `A` y `B` mediante accesos coalescentes a memoria global y luego reutilicen esos datos desde memoria compartida, reduciendo dr谩sticamente el n煤mero de accesos globales. Con `n = 16384`, la implementaci贸n en GPU b谩sica demora **30.89 segundos**, mientras que la versi贸n que aprovecha la shared memory tarda **10.51 segundos**.

![](./img/Speedup-GPU-vs-CPU.png)

### Multiplicaci贸n de matrices con tensor cores

Finalmente, la raz贸n por la que la versi贸n con **GPU tensor cores** supera a las dem谩s es que aprovecha **unidades de c贸mputo especializadas** dise帽adas para la **multiplicaci贸n de matrices de tama帽o fijo**. Estas unidades pueden realizar una cantidad significativamente mayor de operaciones de punto flotante por ciclo de reloj que los CUDA cores convencionales.

La multiplicaci贸n se ejecuta de forma **cooperativa a nivel de warp** (unidad de 32 hilos), utilizando **fragmentos WMMA** de tama帽o fijo. Mediante la instrucci贸n `mma_sync`, un warp completo realiza la operaci贸n entre dos bloques (1616) de las matrices de entrada y acumula el resultado en un bloque del mismo tama帽o, reemplazando m煤ltiples operaciones aritm茅ticas escalares por una 煤nica instrucci贸n especializada.

Adem谩s, la matriz de entrada `B` se transpone previamente para favorecer **accesos coalescentes a memoria**.

Es importante notar que parte de la aceleraci贸n obtenida con tensor cores se debe a la precisi贸n reducida, ya que las matrices de entrada se representan en formato **half (16 bits)** en lugar de **float (32 bits)**, como son originalmente recibidas. La tarjeta utilizada **RTX 2060 Super** presenta un rendimiento te贸rico aproximado de **14.36 TFLOPS en FP16**, frente a **7.181 TFLOPS en FP32**, por lo que la conversi贸n a FP16 permite al programa aprovechar este rendimiento ventajoso. Esta conversi贸n introduce un overhead adicional en el tiempo de ejecuci贸n medido, que podr铆a evitarse si las entradas ya estuvieran originalmente en dicho formato.


## Hardware utilizado para los tests
### CPU
|Especificaciones CPU||
|-|-|
|Marca|AMD|
|Modelo|Ryzen 5 3600|
|Frecuencia|3.6 GHz|
|Frecuencia turbo|Hasta 4.2 GHz|
|N煤cleos|6|
|Hilos|12|
|Cache L1|64 KB (por n煤cleo)|
|Cache L2|512 KB (por n煤cleo)|
|Cache L3|32 MB (compartido)|

### RAM
|Especificaciones RAM||
|-|-|
|Capacidad|32 GB (_Dual channel, 4x8 GB_)|
|Tipo|DDR4|
|Frecuencia|3000 MHz|

### GPU
|Especificaciones GPU||
|-|-|
|Marca|NVIDIA|
|Modelo|RTX 2060 Super|
|Procesador gr谩fico|TU106|
|Arquitectura|_Turing_|
|Reloj base|1470 MHz|
|Reloj boost|1650 MHz|
|VRAM|8 GB|
|Tipo VRAM|GDDR6|
|Ancho de banda VRAM|448.0 GB/s|
|CUDA Cores|2176|
|Tensor Cores|272|
|Tensor Cores por SM|8|
|Rendimiento FP16 (half)|14.36 TFLOPS|
|Rendimiento FP32 (float)|7.181 TFLOPS|
|Rendimiento FP64 (double)|224.4 GFLOPS|
|Versi贸n CUDA|7.5|

---

## Referencias
#### Shared memory
- [Mutliplicaci贸n matrices memoria compartida](https://medium.com/@dhanushg295/mastering-cuda-matrix-multiplication-an-introduction-to-shared-memory-tile-memory-coalescing-and-d7979499b9c5)

#### Tensor cores
- [Documentaci贸n oficial de Nvidia sobre Tensor cores](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)
- [Blog explicativo sobre la programaci贸n sobre Tensor cores](https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/)
- [Video explicativo sobre programaci贸n en Tensor cores](https://youtu.be/Yt1A-vaWTck?si=3m5EwRk-dz3hGI70)

#### Hardware
- [Especificaciones RTX 2060 Super](https://www.techpowerup.com/gpu-specs/geforce-rtx-2060-super.c3441)
- [Especificaciones Ryzen 5 3600](https://www.techpowerup.com/cpu-specs/ryzen-5-3600.c2132)
